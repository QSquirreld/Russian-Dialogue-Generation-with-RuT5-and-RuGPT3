{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5cd6bd1e29330f4b",
      "metadata": {
        "id": "5cd6bd1e29330f4b"
      },
      "source": [
        "# Задание\n",
        "\n",
        "Обучить decoder-only модель предсказывать ответ системы на запрос пользователя.\n",
        "\n",
        "1. Проверить работу модели в сценарии близком к выбранному датасету в zero-shot режиме.\n",
        "2. Посчитать метрики bleu и meteor исходной модели на валидационном сабсете выбранного датасета.\n",
        "3. Дообучить модель на обучающем сабсете выбранного датасета.\n",
        "4. Проверить работу модели в сценарии близком к выбранному датасету.\n",
        "5. Посчитать метрики bleu и meteor обученной модели на валидационном сабсете выбранного датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T12:12:06.620071Z",
          "start_time": "2024-03-26T12:12:06.596759Z"
        },
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "70aa2bcd0ce59da6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T12:12:07.959941Z",
          "start_time": "2024-03-26T12:12:07.937849Z"
        },
        "id": "70aa2bcd0ce59da6"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hVUuMYZ8dsFs",
      "metadata": {
        "id": "hVUuMYZ8dsFs"
      },
      "source": [
        "*По идеи должно улучшить обучение*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "343887db7d04bbcb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T12:12:11.717758Z",
          "start_time": "2024-03-26T12:12:11.567958Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "343887db7d04bbcb",
        "outputId": "5295bf2f-71cd-4e83-d559-58c24b0643a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "special_tokens_dict = {\n",
        "    \"additional_special_tokens\": [\"<user>\", \"<bot>\"]\n",
        "}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e92b8c495dc023a3",
      "metadata": {
        "id": "e92b8c495dc023a3"
      },
      "source": [
        "# Тестовый запуск"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "7498ec4c868c47ee",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-25T22:53:46.011070Z",
          "start_time": "2024-03-25T22:53:46.004069Z"
        },
        "id": "7498ec4c868c47ee"
      },
      "outputs": [],
      "source": [
        "text = 'Привет! Расскажи, как твои дела?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "KL9E3f302YCp",
      "metadata": {
        "id": "KL9E3f302YCp"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(text, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "zdDa6DEl2WSf",
      "metadata": {
        "id": "zdDa6DEl2WSf"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    hypotheses = model.generate(\n",
        "        **inputs,\n",
        "        temperature=0.9,\n",
        "        do_sample=True,\n",
        "        top_p=0.7,\n",
        "        num_return_sequences=3,\n",
        "        repetition_penalty=2.5,\n",
        "        max_length=32,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "838_xU5t2cNv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "838_xU5t2cNv",
        "outputId": "ea38499d-7056-4d95-b651-117d60dd22f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Привет! Расскажи, как твои дела?\n",
            "А ты откуда знаешь что у меня дома кто-то есть. Что я на даче живу??) Приветик))\n",
            "Привет! Расскажи, как твои дела?\n",
            "Я не могу. И я с тобой тоже... но пока ничего плохого сказать тебе нельзя: в последнее время ты стала\n",
            "Привет! Расскажи, как твои дела?\n",
            "Смотря что за человек. Если не секрет - сколько ему лет и где он живет в Москве (может с родителями\n"
          ]
        }
      ],
      "source": [
        "for h in hypotheses:\n",
        "    print(tokenizer.decode(h, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a525955449fae8f5",
      "metadata": {
        "id": "a525955449fae8f5"
      },
      "source": [
        "# Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "2cdace7cd5acc688",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T12:07:50.405590Z",
          "start_time": "2024-03-26T12:07:49.622285Z"
        },
        "id": "2cdace7cd5acc688"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "52be007b",
      "metadata": {
        "id": "52be007b"
      },
      "outputs": [],
      "source": [
        "df = load_dataset(\"zjkarina/matreshka\")\n",
        "train = df['train'].to_pandas()\n",
        "test, valid = train_test_split(df['validation'].to_pandas(), test_size=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-derJnudyvu",
      "metadata": {
        "id": "9-derJnudyvu"
      },
      "source": [
        "**Вспомогательные функции**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "qP60fFZdxk70",
      "metadata": {
        "id": "qP60fFZdxk70"
      },
      "outputs": [],
      "source": [
        "def _df_to_gpt_format(df):\n",
        "\n",
        "    dialogs = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        role = row['role']\n",
        "        dialog = row['dialog']\n",
        "\n",
        "        if role is None or dialog is None or len(role) < 2:\n",
        "            continue\n",
        "\n",
        "        role = [r.lower() for r in role]\n",
        "\n",
        "        if any(r not in ['user', 'bot'] for r in role):\n",
        "            continue\n",
        "\n",
        "        role, dialog = _drop_first_bot_last_user_utterings(role, dialog)\n",
        "\n",
        "        if len(role) < 2:\n",
        "            continue\n",
        "\n",
        "        role, dialog = _shrink_roles_and_dialogue(role, dialog)\n",
        "\n",
        "        # формат для GPT\n",
        "        text = ''\n",
        "        for r, d in zip(role, dialog):\n",
        "            prefix = '<user>' if r == 'user' else '<bot>'\n",
        "            text += f'{prefix}: {d.strip()}\\n'\n",
        "\n",
        "        dialogs.append(text.strip() + \" </s>\")\n",
        "\n",
        "    return dialogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "218NaakwxzCO",
      "metadata": {
        "id": "218NaakwxzCO"
      },
      "outputs": [],
      "source": [
        "def _shrink_roles_and_dialogue(row_role, row_dialog):\n",
        "    shrinked_role = []\n",
        "    shrinked_dialog = []\n",
        "    prev_role = ''\n",
        "    for i in range(len(row_dialog)):\n",
        "        if row_role[i] != prev_role:\n",
        "            shrinked_role.append(row_role[i])\n",
        "            shrinked_dialog.append(row_dialog[i])\n",
        "        else:\n",
        "            shrinked_dialog[-1] += ' ' + row_dialog[i]\n",
        "        prev_role = row_role[i]\n",
        "\n",
        "    assert len(shrinked_dialog) == len(shrinked_role)\n",
        "\n",
        "    shrinked_role, shrinked_dialog = _drop_first_bot_last_user_utterings(shrinked_role, shrinked_dialog)\n",
        "\n",
        "    return shrinked_role, shrinked_dialog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "HIPPRnI7x0ek",
      "metadata": {
        "id": "HIPPRnI7x0ek"
      },
      "outputs": [],
      "source": [
        "def _drop_first_bot_last_user_utterings(role, dialog):\n",
        "    if role[-1] == 'user':\n",
        "        role, dialog = role[:-1], dialog[:-1]\n",
        "    if role[0] == 'bot':\n",
        "        role, dialog = role[1:], dialog[1:]\n",
        "    return role, dialog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "CUvjcqU4x1jj",
      "metadata": {
        "id": "CUvjcqU4x1jj"
      },
      "outputs": [],
      "source": [
        "format_train = _df_to_gpt_format(train)\n",
        "format_val = _df_to_gpt_format(valid)\n",
        "format_test = _df_to_gpt_format(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "XFH2lG4Ox2lY",
      "metadata": {
        "id": "XFH2lG4Ox2lY"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(format_train, columns=['text'])\n",
        "df_val = pd.DataFrame(format_val, columns=['text'])\n",
        "df_test = pd.DataFrame(format_test, columns=['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ig28OzvUdb0g",
      "metadata": {
        "id": "Ig28OzvUdb0g"
      },
      "source": [
        "**Токенизация для GPT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "N-p1pmslx4Sc",
      "metadata": {
        "id": "N-p1pmslx4Sc"
      },
      "outputs": [],
      "source": [
        "def tokenize_data(df, tokenizer, max_length=128):\n",
        "    tokens = tokenizer.batch_encode_plus(\n",
        "        df[\"text\"].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",   # или \"longest\" если не хочешь фиксированный размер\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = tokens['input_ids'].clone()\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "    tokens['labels'] = labels\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "Nvy4oNAkx5tx",
      "metadata": {
        "id": "Nvy4oNAkx5tx"
      },
      "outputs": [],
      "source": [
        "train_tokens = tokenize_data(df_train, tokenizer)\n",
        "val_tokens   = tokenize_data(df_val, tokenizer)\n",
        "test_tokens  = tokenize_data(df_test, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "MTssCNfox613",
      "metadata": {
        "id": "MTssCNfox613"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    train_tokens['input_ids'],\n",
        "    train_tokens['attention_mask'],\n",
        "    train_tokens['labels']\n",
        ")\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    val_tokens['input_ids'],\n",
        "    val_tokens['attention_mask'],\n",
        "    val_tokens['labels']\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "  test_tokens['input_ids'],\n",
        "  test_tokens['attention_mask'],\n",
        "  test_tokens['labels']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "hTMuYUGnx8GU",
      "metadata": {
        "id": "hTMuYUGnx8GU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=8, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "BWWbiniBx9AW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWWbiniBx9AW",
        "outputId": "3c14e444-5cda-43f8-b273-727fb622d979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'mps' if torch.backends.mps.is_built() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "6779RJ3yx-Wy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6779RJ3yx-Wy",
        "outputId": "aa354e8a-290d-4b75-b670-f26ce03abede"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50259, 768)\n",
              "    (wpe): Embedding(2048, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "qOqH5cfxx_53",
      "metadata": {
        "id": "qOqH5cfxx_53"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rHGcTuVkdWKm",
      "metadata": {
        "id": "rHGcTuVkdWKm"
      },
      "source": [
        "**Функции обучения**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "I1jk3T7lyA_g",
      "metadata": {
        "id": "I1jk3T7lyA_g"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, loader, epoch, num_epochs, optimizer, mode):\n",
        "    total_loss = 0\n",
        "    is_train = (mode == 'Training')\n",
        "    model.train() if is_train else model.eval()\n",
        "\n",
        "    loop = tqdm(loader, desc=f\"{mode} epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for input_ids, attention_mask, labels in loop:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"{mode} epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "SZcX8XiGyCaT",
      "metadata": {
        "id": "SZcX8XiGyCaT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    num_epochs=5,\n",
        "):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_epoch(model, train_loader, epoch, num_epochs, optimizer, mode='Training')\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_epoch(model, val_loader, epoch, num_epochs, optimizer, mode='Validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LCxI7P-BdSI5",
      "metadata": {
        "id": "LCxI7P-BdSI5"
      },
      "source": [
        "**Обучение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "cOWoqTWbyEm9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOWoqTWbyEm9",
        "outputId": "5568717c-c6b2-48b1-c813-f2046dccd59a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training epoch 1/5: 100%|██████████| 832/832 [04:13<00:00,  3.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch 1/5: Loss = 2.1267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation epoch 1/5: 100%|██████████| 104/104 [00:09<00:00, 11.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation epoch 1/5: Loss = 1.7702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training epoch 2/5: 100%|██████████| 832/832 [04:12<00:00,  3.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch 2/5: Loss = 1.7482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation epoch 2/5: 100%|██████████| 104/104 [00:09<00:00, 11.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation epoch 2/5: Loss = 1.7114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training epoch 3/5: 100%|██████████| 832/832 [04:11<00:00,  3.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch 3/5: Loss = 1.6329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation epoch 3/5: 100%|██████████| 104/104 [00:09<00:00, 11.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation epoch 3/5: Loss = 1.6932\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training epoch 4/5: 100%|██████████| 832/832 [04:12<00:00,  3.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch 4/5: Loss = 1.5420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation epoch 4/5: 100%|██████████| 104/104 [00:09<00:00, 11.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation epoch 4/5: Loss = 1.6850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training epoch 5/5: 100%|██████████| 832/832 [04:12<00:00,  3.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch 5/5: Loss = 1.4654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation epoch 5/5: 100%|██████████| 104/104 [00:09<00:00, 11.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation epoch 5/5: Loss = 1.6802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(model, train_loader, val_loader, optimizer, num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H3a8WaBdeCq7",
      "metadata": {
        "id": "H3a8WaBdeCq7"
      },
      "source": [
        "# Получение ответов от FT модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "NPhvih0DyFsR",
      "metadata": {
        "id": "NPhvih0DyFsR"
      },
      "outputs": [],
      "source": [
        "inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "inputs['attention_mask'] = inputs['attention_mask'].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "_y8DCk8ryHDV",
      "metadata": {
        "id": "_y8DCk8ryHDV"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\n",
        "    hypotheses = model.generate(\n",
        "        **inputs,\n",
        "        temperature=0.9,\n",
        "        do_sample=True,             # sampling or greedy decoding\n",
        "        # (at each decoding step selects the token with the highest prob without considering the impact on future tokens)\n",
        "        top_p=0.7,\n",
        "        num_return_sequences=3,\n",
        "        repetition_penalty=2.5,     # https://arxiv.org/pdf/1909.05858.pdf\n",
        "        max_length=32,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "rAOqm8FEyJVq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAOqm8FEyJVq",
        "outputId": "98bcee69-9c11-42a3-901f-2cadc62f55fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Привет! Расскажи, как твои дела?\n",
            ": Привет. У меня все хорошо и спасибо за спрос о курсах программирования для начинающих... А у тебя есть какие\n",
            "Привет! Расскажи, как твои дела?\n",
            ": Привет. У меня все хорошо с работой и учебой в университете Сейчас учусь на факультете журналистики В\n",
            "Привет! Расскажи, как твои дела?\n",
            ": Привет. У меня все хорошо а у тебя что нового интересного происходит в жизни/в работе или учебе\n"
          ]
        }
      ],
      "source": [
        "for h in hypotheses:\n",
        "    print(tokenizer.decode(h, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "4aQVemuFyK5J",
      "metadata": {
        "id": "4aQVemuFyK5J"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, max_new_tokens=50, num_return_sequences=1):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    return [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ebb2982462fb9b",
      "metadata": {
        "id": "6ebb2982462fb9b"
      },
      "source": [
        "# Вычисление BLEU-метрик на валидационных данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "QO54TZqCyLt4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO54TZqCyLt4",
        "outputId": "0a8ba6b7-664d-4b55-e85a-8697353cdd93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "5kF4Gt0m3uce",
      "metadata": {
        "id": "5kF4Gt0m3uce"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from torchmetrics.text import BLEUScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "6b9d92b230477dbe",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T13:01:51.831041Z",
          "start_time": "2024-03-26T13:01:40.367030Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b9d92b230477dbe",
        "outputId": "c88b5b3e-aa33-41b6-b38d-a19ea7f18572"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [02:28<00:00,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg bleu: 0.0976013445481658\n",
            "avg initial bleu: 0.0976013445481658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from torchmetrics.text import BLEUScore\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "bleu = BLEUScore(n_gram=1)\n",
        "cnt = 100\n",
        "\n",
        "initial_model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\").to(device)\n",
        "finetuned_model = model\n",
        "\n",
        "initial_bleu_score = 0.\n",
        "avg_bleu_score = 0.\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _, row in tqdm(df_val.head(cnt).iterrows(), total=cnt):\n",
        "        prompt = row['text']\n",
        "        target = row['text'].split('<bot>')[-1].strip()\n",
        "\n",
        "        init_pred = generate(initial_model, tokenizer, prompt)[0]\n",
        "        fine_pred = generate(finetuned_model, tokenizer, prompt)[0]\n",
        "\n",
        "        # Обрезаем до первого предложения (по желанию)\n",
        "        target_clean = re.split(r'\\.|!|\\?', target)[0]\n",
        "        init_clean = re.split(r'\\.|!|\\?', init_pred)[0]\n",
        "        fine_clean = re.split(r'\\.|!|\\?', fine_pred)[0]\n",
        "\n",
        "        initial_bleu_score += bleu(init_clean, target_clean).item()\n",
        "        avg_bleu_score += bleu(fine_clean, target_clean).item()\n",
        "\n",
        "print('avg bleu:', avg_bleu_score * 1. / cnt)\n",
        "print('avg initial bleu:',  initial_bleu_score * 1. / cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e359e73a2b5ad983",
      "metadata": {
        "id": "e359e73a2b5ad983"
      },
      "source": [
        "# Вычисление METEOR-метрик на валидационных данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "boFF4G9Ty-c-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boFF4G9Ty-c-",
        "outputId": "b322badf-7218-46ee-8621-e38885b03f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "154730fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "9ba5e537083b4c96bf05e518fd2dd5c0",
            "36a3657583024eb0978cab1be9ebe2a3",
            "058c507a70e44405a094e5d8e2e110de",
            "e576283d5fb241c0a59c8b7a2154cb39",
            "61650edead5542248a0799e1554be4ab",
            "62c84ff995fb41cc87dc116d29e515c1",
            "d0cb87c65dad46539f30f9486b93574b",
            "3d00fee93fd24042a6a27f0544c14523",
            "47783bd7f6bd4ae493490f464db72a72",
            "088f47efa5394a8286e8f220f926e193",
            "4bc1601211784eb7afc0de0c2a1ed815"
          ]
        },
        "id": "154730fd",
        "outputId": "1eec7c56-e6c4-4a41-b998-4096a2e8cde3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ba5e537083b4c96bf05e518fd2dd5c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "meteor = evaluate.load('meteor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "f3edc5a4f07aded9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T20:57:22.051224Z",
          "start_time": "2024-03-26T20:57:02.860421Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3edc5a4f07aded9",
        "outputId": "b02f2a2f-179d-4238-94a0-60e933984300"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [02:09<00:00,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg meteor: 0.14646473518046407\n",
            "avg initial meteor: 0.14693380355737207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cnt = 100\n",
        "\n",
        "initial_model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\").to(device)\n",
        "finetuned_model = model\n",
        "\n",
        "avg_meteor_score = 0.\n",
        "initial_meteor_score = 0.\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _, row in tqdm(df_val.head(cnt).iterrows(), total=cnt):\n",
        "        prompt = row['text']\n",
        "        target = row['text'].split('<bot>')[-1].strip()\n",
        "\n",
        "        pred_finetuned = generate(finetuned_model, tokenizer, prompt, num_return_sequences=1)[0]\n",
        "        pred_initial = generate(initial_model, tokenizer, prompt, num_return_sequences=1)[0]\n",
        "\n",
        "        target_clean = re.split(r'\\.|!|\\?', target)[0]\n",
        "        pred_finetuned_clean = re.split(r'\\.|!|\\?', pred_finetuned)[0]\n",
        "        pred_initial_clean = re.split(r'\\.|!|\\?', pred_initial)[0]\n",
        "\n",
        "        metric = meteor.compute(predictions=[pred_finetuned_clean], references=[target_clean])\n",
        "        initial_metric = meteor.compute(predictions=[pred_initial_clean], references=[target_clean])\n",
        "\n",
        "        avg_meteor_score += metric['meteor']\n",
        "        initial_meteor_score += initial_metric['meteor']\n",
        "\n",
        "print('avg meteor:', avg_meteor_score * 1. / cnt)\n",
        "print('avg initial meteor:', initial_meteor_score * 1. / cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d030f4",
      "metadata": {},
      "source": [
        "# Выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8786e171",
      "metadata": {},
      "source": [
        "1. Метрики не изменились, однако визуально, ответ после дообучения точнее.\n",
        "<br>Это может быть связано с тем, что:\n",
        "- метрики не видят изменений, хотя они есть.\n",
        "- модель `rugpt3small` упёрлась в \"потолок\"\n",
        "- данных недостаточно или они менее полезны\n",
        "- необходимо, в сравнении с `RuT5`, дольше обучать модель (увеличить `learning rate`, `tokenizer size`, `batch_size`)\n",
        "- как вариант, можно убрать токены `<user>`, `<bot>`. Возможно это запутало модель\n",
        "2. Метрики очень низкого уровня\n",
        "- Однако у нас не так много данных для оценки: один эталон и короткие ответы\n",
        "- *Лучший результат полученный мной для `METEOR` был `0.204`, которого всё равно недостаточно*\n",
        "3. Ответ после дообучения действительно похож на диалог, в отличии от первоначального ответа, однако местами присутствуют галлюцинации и нерелевантная информация\n",
        "- Вариантом решения может стать применение большей модели напр. `rugpt3medium`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "058c507a70e44405a094e5d8e2e110de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d00fee93fd24042a6a27f0544c14523",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47783bd7f6bd4ae493490f464db72a72",
            "value": 1
          }
        },
        "088f47efa5394a8286e8f220f926e193": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36a3657583024eb0978cab1be9ebe2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62c84ff995fb41cc87dc116d29e515c1",
            "placeholder": "​",
            "style": "IPY_MODEL_d0cb87c65dad46539f30f9486b93574b",
            "value": "Downloading builder script: "
          }
        },
        "3d00fee93fd24042a6a27f0544c14523": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "47783bd7f6bd4ae493490f464db72a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bc1601211784eb7afc0de0c2a1ed815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61650edead5542248a0799e1554be4ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62c84ff995fb41cc87dc116d29e515c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba5e537083b4c96bf05e518fd2dd5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36a3657583024eb0978cab1be9ebe2a3",
              "IPY_MODEL_058c507a70e44405a094e5d8e2e110de",
              "IPY_MODEL_e576283d5fb241c0a59c8b7a2154cb39"
            ],
            "layout": "IPY_MODEL_61650edead5542248a0799e1554be4ab"
          }
        },
        "d0cb87c65dad46539f30f9486b93574b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e576283d5fb241c0a59c8b7a2154cb39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_088f47efa5394a8286e8f220f926e193",
            "placeholder": "​",
            "style": "IPY_MODEL_4bc1601211784eb7afc0de0c2a1ed815",
            "value": " 7.02k/? [00:00&lt;00:00, 191kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
